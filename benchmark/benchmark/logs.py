from datetime import datetime
from glob import glob
from multiprocessing import Pool
from os.path import join
from re import findall, search
from statistics import mean

from benchmark.utils import Print


class ParseError(Exception):
    pass


class LogParser:
    def __init__(self, clients, nodes, faults, protocol, ddos):
        inputs = [clients, nodes]
        assert all(isinstance(x, list) for x in inputs)
        assert all(isinstance(x, str) for y in inputs for x in y)
        assert all(x for x in inputs)

        self.protocol = protocol
        self.ddos = ddos
        self.faults = faults
        self.committee_size = len(nodes) + faults

        # Parse the clients logs.
        try:
            with Pool() as p:
                results = p.map(self._parse_clients, clients)
        except (ValueError, IndexError) as e:
            raise ParseError(f'Failed to parse client logs: {e}')
        self.size, self.rate, self.start, misses, self.sent_samples \
            = zip(*results)
        self.misses = sum(misses)

        # Parse the nodes logs.
        try:
            with Pool() as p:
                results = p.map(self._parse_nodes, nodes)
        except (ValueError, IndexError) as e:
            raise ParseError(f'Failed to parse node logs: {e}')
        proposals, commits, sizes, self.received_samples, timeouts, self.configs, d2t_samples, fallback_infos \
            = zip(*results)
        self.proposals = self._merge_results([x.items() for x in proposals])
        self.commits = self._merge_results([x.items() for x in commits])
        self.sizes = {
            k: v for x in sizes for k, v in x.items() if k in self.commits
        }
        self.timeouts = max(timeouts)
        self.d2t_samples = self._merge_results([x.items() for x in d2t_samples])

        # Merge fallback information from all nodes
        self.fallback_info = {}
        for fallback_dict in fallback_infos:
            for digest, fallback_val in fallback_dict.items():
                if digest not in self.fallback_info:
                    self.fallback_info[digest] = fallback_val

        # Check whether clients missed their target rate.
        if self.misses != 0:
            Print.warn(
                f'Clients missed their target rate {self.misses:,} time(s)'
            )

        # Check whether the nodes timed out.
        # Note that nodes are expected to time out once at the beginning.
        if self.timeouts > 1:
            Print.warn(f'Nodes timed out {self.timeouts:,} time(s)')

    def _merge_results(self, input):
        # Keep the earliest timestamp.
        merged = {}
        for x in input:
            for k, v in x:
                if not k in merged or merged[k] > v:
                    merged[k] = v
        return merged

    def _parse_clients(self, log):
        if search(r'Error', log) is not None:
            raise ParseError('Client(s) panicked')

        size = int(search(r'Transactions size: (\d+)', log).group(1))
        rate = int(search(r'Transactions rate: (\d+)', log).group(1))

        tmp = search(r'\[(.*Z) .* Start ', log).group(1)
        start = self._to_posix(tmp)

        misses = len(findall(r'rate too high', log))

        tmp = findall(r'\[(.*Z) .* sample transaction (\d+)', log)
        samples = {int(s): self._to_posix(t) for t, s in tmp}

        return size, rate, start, misses, samples

    def _parse_nodes(self, log):
        if search(r'panic', log) is not None:
            raise ParseError('Client(s) panicked sb')

        tmp = findall(r'\[(.*Z) .* Created B\d+-\d+\(([^ ]+)\)', log)
        tmp = [(d, self._to_posix(t)) for t, d in tmp]
        proposals = self._merge_results([tmp])

        tmp = findall(r'\[(.*Z) .* Committed B(\d+)-(\d+)\(([^ ]+)\)', log)
        tmp = [(d, self._to_posix(t)) for t, r, f, d in tmp]
        commits = self._merge_results([tmp])

        # Extract fallback information from committed blocks
        tmp = findall(r'\[(.*Z) .* Committed B(\d+)-(\d+)\(([^ ]+)\)', log)
        fallback_info = {d: int(f) for t, r, f, d in tmp}

        tmp = findall(r'Payload ([^ ]+) contains (\d+) B', log)
        sizes = {d: int(s) for d, s in tmp}

        tmp = findall(r'\[(.*Z) .* Payload ([^ ]+) contains sample tx (\d+)', log)
        samples = {int(s): d for _,d, s in tmp}
        d2t_samples= {d:self._to_posix(t) for t,d,_ in tmp}

        tmp = findall(r'.* WARN .* Timeout', log)
        timeouts = len(tmp)

        configs = {
            'consensus': {
                'timeout_delay': int(
                    search(r'Consensus timeout delay .* (\d+)', log).group(1)
                ),
                'sync_retry_delay': int(
                    search(
                        r'Consensus synchronizer retry delay .* (\d+)', log
                    ).group(1)
                ),
                'max_payload_size': int(
                    search(r'Consensus max payload size .* (\d+)', log).group(1)
                ),
                'min_block_delay': int(
                    search(r'Consensus min block delay .* (\d+)', log).group(1)
                ),
            },
            'mempool': {
                'queue_capacity': int(
                    search(r'Mempool queue capacity set to (\d+)', log).group(1)
                ),
                # 'sync_retry_delay': int(
                #     search(
                #         r'Mempool synchronizer retry delay .* (\d+)', log
                #     ).group(1)
                # ),
                'max_payload_size': int(
                    search(r'Mempool max payload size .* (\d+)', log).group(1)
                ),
                'min_block_delay': int(
                    search(r'Mempool min block delay .* (\d+)', log).group(1)
                ),
            }
        }

        return proposals, commits, sizes, samples, timeouts, configs, d2t_samples, fallback_info

    def _to_posix(self, string):
        x = datetime.fromisoformat(string.replace('Z', '+00:00'))
        return datetime.timestamp(x)

    def _consensus_throughput(self):
        if not self.commits:
            return 0, 0, 0
        start, end = min(self.proposals.values()), max(self.commits.values())
        duration = end - start
        bytes = sum(self.sizes.values())
        bps = bytes / duration
        tps = bps / self.size[0]
        return tps, bps, duration

    def _consensus_latency(self):
        latency = [c - self.proposals[d] for d, c in self.commits.items()]
        return mean(latency) if latency else 0

    def _end_to_end_throughput(self):
        if not self.commits:
            return 0, 0, 0
        start, end = min(self.start), max(self.commits.values())
        duration = end - start
        bytes = sum(self.sizes.values())
        bps = bytes / duration
        tps = bps / self.size[0]
        return tps, bps, duration

    def _end_to_end_latency(self):
        latency = []
        for sent, received in zip(self.sent_samples, self.received_samples):
            for tx_id, batch_id in received.items():
                if batch_id in self.commits:
                    assert tx_id in sent  # We receive txs that we sent.
                    start = sent[tx_id]
                    end = self.commits[batch_id]
                    latency += [end-start]
        return mean(latency) if latency else 0

    def _fallback_statistics(self):
        """Calculate the number and ratio of blocks with fallback 0 and 1."""
        fallback_0_count = 0
        fallback_1_count = 0

        for digest in self.commits.keys():
            if digest in self.fallback_info:
                if self.fallback_info[digest] == 0:
                    fallback_0_count += 1
                elif self.fallback_info[digest] == 1:
                    fallback_1_count += 1

        total_count = fallback_0_count + fallback_1_count
        fallback_0_ratio = (fallback_0_count / total_count * 100) if total_count > 0 else 0
        fallback_1_ratio = (fallback_1_count / total_count * 100) if total_count > 0 else 0

        return fallback_0_count, fallback_1_count, total_count, fallback_0_ratio, fallback_1_ratio

    def result(self):
        consensus_latency = self._consensus_latency() * 1000
        consensus_tps, consensus_bps, _ = self._consensus_throughput()
        end_to_end_tps, end_to_end_bps, duration = self._end_to_end_throughput()
        end_to_end_latency = self._end_to_end_latency() * 1000

        # Get fallback statistics
        fallback_0_count, fallback_1_count, total_count, fallback_0_ratio, fallback_1_ratio = self._fallback_statistics()

        consensus_timeout_delay = self.configs[0]['consensus']['timeout_delay']
        consensus_sync_retry_delay = self.configs[0]['consensus']['sync_retry_delay']
        consensus_max_payload_size = self.configs[0]['consensus']['max_payload_size']
        consensus_min_block_delay = self.configs[0]['consensus']['min_block_delay']
        mempool_queue_capacity = self.configs[0]['mempool']['queue_capacity']
        # mempool_sync_retry_delay = self.configs[0]['mempool']['sync_retry_delay']
        mempool_max_payload_size = self.configs[0]['mempool']['max_payload_size']
        mempool_min_block_delay = self.configs[0]['mempool']['min_block_delay']

        return (
            '\n'
            '-----------------------------------------\n'
            ' SUMMARY:\n'
            '-----------------------------------------\n'
            ' + CONFIG:\n'
            f' Protocol: {self.protocol} \n'
            f' DDOS attack: {self.ddos} \n'
            f' Committee size: {self.committee_size} nodes\n'
            f' Input rate: {sum(self.rate):,} tx/s\n'
            f' Transaction size: {self.size[0]:,} B\n'
            f' Faults: {self.faults} nodes\n'
            f' Execution time: {round(duration):,} s\n'
            '\n'
            f' Consensus timeout delay: {consensus_timeout_delay:,} ms\n'
            f' Consensus sync retry delay: {consensus_sync_retry_delay:,} ms\n'
            f' Consensus max payloads size: {consensus_max_payload_size:,} B\n'
            f' Consensus min block delay: {consensus_min_block_delay:,} ms\n'
            f' Mempool queue capacity: {mempool_queue_capacity:,} B\n'
            # f' Mempool sync retry delay: {mempool_sync_retry_delay:,} ms\n'
            f' Mempool max payloads size: {mempool_max_payload_size:,} B\n'
            f' Mempool min block delay: {mempool_min_block_delay:,} ms\n'
            '\n'
            ' + RESULTS:\n'
            f' Consensus TPS: {round(consensus_tps):,} tx/s\n'
            f' Consensus BPS: {round(consensus_bps):,} B/s\n'
            f' Consensus latency: {round(consensus_latency):,} ms\n'
            '\n'
            f' End-to-end TPS: {round(end_to_end_tps):,} tx/s\n'
            f' End-to-end BPS: {round(end_to_end_bps):,} B/s\n'
            f' End-to-end latency: {round(end_to_end_latency):,} ms\n'
            '\n'
            ' + FALLBACK STATISTICS:\n'
            f' Total committed blocks: {total_count:,}\n'
            f' Blocks with fallback=0: {fallback_0_count:,} ({fallback_0_ratio:.2f}%)\n'
            f' Blocks with fallback=1: {fallback_1_count:,} ({fallback_1_ratio:.2f}%)\n'
            '-----------------------------------------\n'
        )

    def latencyWithTime(self):
        t2d,c_times = {},[]#time to digest
        for d,t in self.commits.items():
            t2d.setdefault(t,[]).append(d)
            c_times+=[t]

        c_times.sort() # 按提交时间排序
        latencyWTime = []

        for t in c_times:
            txs = t2d[t]
            p_times = [] # 记录create时间
            ct2d = {}
            for d in txs:
                pt = self.proposals[d]
                ct2d.setdefault(pt,[]).append(d)
                p_times.append(pt)
            p_times.sort() # 内部按created 时间排序
            for pt in p_times:
                txs = ct2d[pt]
                for d in txs:
                    if d in self.d2t_samples:
                        start = self.d2t_samples[d]
                        end = self.commits[d]
                        latencyWTime.append((d,end-start))
        content ,seq= "",0
        for d,t in latencyWTime:
            num = self.sizes[d]/self.size[0]
            content += f'{seq},{t},{num}\n'
            seq+=1

        return content+"\n"


    def transactionsWithTime(self):
        times,t_times = [],[0]
        time2num = {}
        for k,t in self.commits.items():
            if k in self.sizes:
                if t not in time2num:
                    time2num[t] = 0
                    times.append(t)
                time2num[t] += self.sizes[k]/self.size[0]

        times.sort()
        start = min(self.proposals.values())    
        times.insert(0,start)
        time2num[start] = 0

        key,temp = 0.0 ,{}
        for t in times:
            d = t-start
            if d<=0.002:
                d=0
            key+=d
            start+=d
            if len(t_times)==0 or t_times[-1]!=key:
                t_times.append(key)    
            temp[key] = temp.get(key-d,0) + time2num[t]

        content = ""
        for i,t in enumerate(t_times):
            line = ""
            if i!=0:
                line += f'{t},{temp[t_times[i-1]]}\n'
            line += f'{t},{temp[t]}\n'
            content+=line
        content += "\n"
        return content
    
    def fallback_stats(self):
        """Generate fallback statistics report."""
        fallback_0_count, fallback_1_count, total_count, fallback_0_ratio, fallback_1_ratio = self._fallback_statistics()

        content = (
            'FALLBACK STATISTICS\n'
            '===================\n'
            f'Total committed blocks: {total_count}\n'
            f'Blocks with fallback=0: {fallback_0_count} ({fallback_0_ratio:.2f}%)\n'
            f'Blocks with fallback=1: {fallback_1_count} ({fallback_1_ratio:.2f}%)\n'
            '\n'
        )
        return content

    def print(self, r_filename, t_filename, l_filename, f_filename=None):
        assert isinstance(r_filename, str)
        assert isinstance(t_filename, str)
        with open(r_filename, 'a') as f:
            f.write(self.result())
        with open(t_filename, 'a') as f:
            f.write(self.transactionsWithTime())
        with open(l_filename, 'a') as f:
            f.write(self.latencyWithTime())
        if f_filename is not None:
            assert isinstance(f_filename, str)
            with open(f_filename, 'a') as f:
                f.write(self.fallback_stats())

    @classmethod
    def process(cls, directory, faults=0, protocol=0, ddos=False):
        assert isinstance(directory, str)

        clients = []
        for filename in sorted(glob(join(directory, 'client-*.log'))):
            with open(filename, 'r') as f:
                clients += [f.read()]
        nodes = []
        for filename in sorted(glob(join(directory, 'node-*.log'))):
            with open(filename, 'r') as f:
                nodes += [f.read()]
                
        return cls(clients, nodes, faults=faults, protocol=protocol, ddos=ddos)
